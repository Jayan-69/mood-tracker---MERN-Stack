import React, { useEffect, useRef, useState, useCallback } from 'react';
import '@tensorflow-models/hand-pose-detection';
import * as handPoseDetection from '@tensorflow-models/hand-pose-detection';

const MoodTracker = () => {
  const [mood, setMood] = useState('');
  const [emotionScores, setEmotionScores] = useState({});
  const [notes, setNotes] = useState('');
  const [history, setHistory] = useState([]);
  // These state variables are kept for potential future use, but not actively used in simplified mode
  const [faceModel, setFaceModel] = useState(null);
  const [handModel, setHandModel] = useState(null);
  const [isVideoReady, setIsVideoReady] = useState(false);
  const [modelsLoaded, setModelsLoaded] = useState(false);
  const [faceDetails, setFaceDetails] = useState(null);
  const [handDetails, setHandDetails] = useState(null);
  const [showLandmarks, setShowLandmarks] = useState(false); // Set to false in simplified mode
  const [error, setError] = useState('');
  const [isDetecting, setIsDetecting] = useState(false);
  
  const videoRef = useRef();
  const canvasRef = useRef();
  const detectionIntervalRef = useRef();

  // Note: Bootstrap CSS is now imported in index.js

  // Setup canvas when video is ready
  const setupCanvas = () => {
    if (canvasRef.current && videoRef.current) {
      const canvas = canvasRef.current;
      canvas.width = videoRef.current.videoWidth || 640;
      canvas.height = videoRef.current.videoHeight || 480;
    }
  };
  
  // Start video stream - define this first so it can be used in other functions
  const startVideo = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { 
          width: { ideal: 640 }, 
          height: { ideal: 480 },
          facingMode: 'user'
        } 
      });
      
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        videoRef.current.onloadedmetadata = () => {
          setIsVideoReady(true);
          setupCanvas();
          console.log('Video stream ready');
        };
      }
    } catch (err) {
      console.error('Error accessing webcam:', err);
      setError('Unable to access webcam. Please check permissions and try again.');
    }
  }, []);

  // Load TensorFlow.js and AI models for face and hand detection
  const loadLibraries = useCallback(() => {
    // First clear any previous error message
    setError('');
    
    // Create script loading functions
    const loadScript = (src) => {
      return new Promise((resolve, reject) => {
        // Check if script is already loaded
        const existingScript = document.querySelector(`script[src="${src}"]`);
        if (existingScript) {
          resolve();
          return;
        }
        
        const script = document.createElement('script');
        script.src = src;
        script.onload = () => resolve();
        script.onerror = () => reject(new Error(`Failed to load script: ${src}`));
        document.body.appendChild(script);
        
        // Add timeout to prevent hanging
        setTimeout(() => {
          if (!script.onload) {
            reject(new Error(`Script load timeout: ${src}`));
          }
        }, 10000); // 10 second timeout
      });
    };
    
    // Load TensorFlow.js core and models in sequence with proper error handling
    loadScript('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js')
      .then(() => {
        console.log('TensorFlow.js core loaded successfully');
        return loadScript('https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.3/dist/face-landmarks-detection.min.js');
      })
      .then(() => {
        console.log('Face model library loaded successfully');
        // Load MediaPipe dependencies first
        return loadScript('https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4.1646424915/hands.min.js');
      })
      .then(() => {
        console.log('MediaPipe Hands solution loaded successfully');
        return loadScript('https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection@2.0.0/dist/hand-pose-detection.min.js');
      })
      .then(() => {
        console.log('Hand detection model loaded successfully');
        return startVideo();
      })
      .then(() => {
        console.log('Video started, loading AI models...');
        
        return Promise.all([
          // Load face detection model
          window.faceLandmarksDetection.load(
            window.faceLandmarksDetection.SupportedPackages.mediapipeFacemesh,
            { maxFaces: 1 }
          ),
          // Load hand detection model with MediaPipe hands backend
          window.handPoseDetection.createDetector(
            window.handPoseDetection.SupportedModels.MediaPipeHands,
            {
              runtime: 'mediapipe',
              modelType: 'full',
              maxHands: 2,  // Explicitly set to detect up to 2 hands
              solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/hands'
            }
          )
        ]);
      })
      .then(([faceModelLoaded, handModelLoaded]) => {
        console.log('AI models loaded successfully');
        setFaceModel(faceModelLoaded);
        setHandModel(handModelLoaded);
        setModelsLoaded(true);
      })
      .catch(error => {
        console.error('Error loading AI libraries:', error);
        // Still allow video to start in case of model loading error
        startVideo().catch(err => console.error('Video start error:', err));
        setError(`AI detection couldn't be loaded: ${error.message}. Using basic mode.`);
        setModelsLoaded(true); // Still set models as loaded to allow app to function
      });
  }, [startVideo]); // Add startVideo as a dependency
  
  // Helper function to get a random emotion distribution
  // Used in the simplified mode instead of AI detection
  const getRandomEmotions = (cyclePosition) => {
    let emotions;
    
    if (cyclePosition < 0.3) {
      // Bias toward happy during first part of cycle
      emotions = {
        neutral: 0.3 + Math.random() * 0.2,
        happy: 0.4 + Math.random() * 0.3,
        sad: Math.random() * 0.2,
        angry: Math.random() * 0.1,
        surprised: Math.random() * 0.1
      };
    } else if (cyclePosition < 0.6) {
      // Bias toward neutral in middle of cycle
      emotions = {
        neutral: 0.6 + Math.random() * 0.2,
        happy: 0.1 + Math.random() * 0.2,
        sad: 0.1 + Math.random() * 0.1,
        angry: Math.random() * 0.1,
        surprised: Math.random() * 0.1
      };
    } else {
      // More varied emotions in last part of cycle
      emotions = {
        neutral: 0.3 + Math.random() * 0.3,
        happy: 0.1 + Math.random() * 0.3,
        sad: 0.1 + Math.random() * 0.3,
        angry: 0.05 + Math.random() * 0.15,
        surprised: 0.05 + Math.random() * 0.15
      };
    }
    
    // Normalize emotion values
    const total = Object.values(emotions).reduce((sum, val) => sum + val, 0);
    Object.keys(emotions).forEach(key => {
      emotions[key] = emotions[key] / total;
    });
    
    return emotions;
  };

  // AI-powered detection function using TensorFlow.js models
  const runDetection = useCallback(() => {
    if (!videoRef.current || !isVideoReady || 
        videoRef.current.paused || videoRef.current.ended ||
        videoRef.current.readyState < 2) {
      return;
    }
    
    // Force the hand model to detect both hands by improving input processing
    const enhanceVideoForHandDetection = (video) => {
      // Create a temporary canvas to process the video frame
      const tempCanvas = document.createElement('canvas');
      const tempCtx = tempCanvas.getContext('2d');
      tempCanvas.width = video.videoWidth || 640;
      tempCanvas.height = video.videoHeight || 480;
      
      // Draw the video to the canvas with slightly increased contrast
      tempCtx.drawImage(video, 0, 0, tempCanvas.width, tempCanvas.height);
      return tempCanvas;
    };
    
    // Check if we have the AI models loaded
    const useAIMode = faceModel && handModel;
    
    // Set detecting state
    setIsDetecting(true);
    
    if (useAIMode) {
      // Full AI detection using TensorFlow.js
      Promise.all([
        // Face detection
        faceModel.estimateFaces({
          input: videoRef.current,
          returnTensors: false,
          flipHorizontal: false,
          predictIrises: false
        }),
        // Hand detection - use enhanced video to improve detection of both hands
        handModel.estimateHands(enhanceVideoForHandDetection(videoRef.current))
      ])
      .then(([faces, hands]) => {
        // Process face detection results
        if (faces && faces.length > 0) {
          const firstFace = faces[0];
          
          // Get face landmarks and metadata
          const landmarks = firstFace.scaledMesh || firstFace.mesh;
          const boundingBox = firstFace.boundingBox || {
            topLeft: [firstFace.box.xMin, firstFace.box.yMin],
            bottomRight: [firstFace.box.xMax, firstFace.box.yMax]
          };
          const faceConfidence = firstFace.confidence || 0.8;
          
          // Calculate emotion scores from landmarks
          const emotions = calculateEmotionsFromLandmarks(landmarks);
          
          // Update emotion state
          setEmotionScores(emotions);
          const dominantEmotion = Object.keys(emotions).reduce((a, b) => 
            emotions[a] > emotions[b] ? a : b
          );
          setMood(dominantEmotion);
          
          // Update face details for UI
          setFaceDetails({
            facesDetected: faces.length,
            keypoints: landmarks ? landmarks.length : 0,
            boundingBox: boundingBox,
            confidence: faceConfidence,
            fallbackMode: false
          });
          
          // Draw face landmarks if enabled
          if (showLandmarks && canvasRef.current) {
            const ctx = canvasRef.current.getContext('2d');
            ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);
            
            // Draw face landmarks
            if (landmarks) {
              ctx.fillStyle = '#32EEDB';
              landmarks.forEach(point => {
                ctx.beginPath();
                ctx.arc(point[0], point[1], 1, 0, 2 * Math.PI);
                ctx.fill();
    const randomEmotions = getRandomEmotions(cyclePosition);
    
    // Update emotions state
    setEmotionScores(randomEmotions);
    const dominantEmotion = Object.keys(randomEmotions).reduce((a, b) => 
      randomEmotions[a] > randomEmotions[b] ? a : b
    );
    setMood(dominantEmotion);
    
    // Provide simulated face details for the UI
    setFaceDetails({
      facesDetected: 1,
      keypoints: 0,
      boundingBox: null,
      confidence: 0.7 + Math.random() * 0.2,
      fallbackMode: true
    });
    
    // Provide simulated hand details with empty arrays to prevent mapping errors
    setHandDetails({
      handsDetected: 0,
      handedness: [], // Empty array to prevent map errors
      keypoints: [],  // Empty array to prevent map errors
      confidence: [], // Empty array to prevent map errors
      fallbackMode: true
    });
    
    // Clear the canvas (no landmarks in simplified mode)
    if (canvasRef.current && canvasRef.current.getContext) {
      const ctx = canvasRef.current.getContext('2d');
      ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);
    }
    
    setIsDetecting(false);
  };
  
  // Helper functions for emotion detection
  const calculateSadnessScore = (isMouthTurnedDown, mouthCornerDiff, mouthRatio) => {
    let score = 0;
    
    // Strongly weight mouth turned down
    if (isMouthTurnedDown) {
      score += 0.6;
    }
    
    // Add score for uneven mouth corners (sadness often causes asymmetry)
    if (mouthCornerDiff > 3) {
      score += 0.3;
    } else if (mouthCornerDiff > 1.5) {
      score += 0.15;
    }
    
    // Low mouth ratio can indicate sadness (not smiling)
    if (mouthRatio < 2.0) {
      score += 0.2;
    }
    
    return Math.min(0.9, score); // Cap at 0.9
  };
  
  const calculateAngerScore = (eyebrowLowered, eyebrowsAngledInward, leftAngle, rightAngle) => {
    let score = 0;
    
    // Strongly weight lowered eyebrows
    if (eyebrowLowered) {
      score += 0.5;
    }
    
    // Strongly weight angled eyebrows (classic anger expression)
    if (eyebrowsAngledInward) {
      score += 0.4;
    } else if (leftAngle < -10 || rightAngle > 10) {
      // Partial eyebrow angling
      score += 0.25;
    }
    
    return Math.min(0.9, score); // Cap at 0.9
  };
  
  const calculateSurpriseScore = (eyebrowRaised, eyesWideOpen, isMouthOpen) => {
    let score = 0;
    
    // Strongly weight raised eyebrows
    if (eyebrowRaised) {
      score += 0.4;
    }
    
    // Wide open eyes are a strong indicator
    if (eyesWideOpen) {
      score += 0.4;
    }
    
    // Open mouth is also a strong indicator
    if (isMouthOpen) {
      score += 0.4;
    }
    
    // All three main indicators present - boost the score
    if (eyebrowRaised && eyesWideOpen && isMouthOpen) {
      score += 0.2;
    }
    
    return Math.min(0.95, score); // Cap at 0.95 - surprise can be very clear
  };
  
  const calculateHappinessScore = (mouthRatio, isMouthTurnedDown) => {
    let score = 0;
    
    // High mouth ratio suggests smiling
    if (mouthRatio > 4.0) {
      score += 0.7;
    } else if (mouthRatio > 3.0) {
      score += 0.5;
    } else if (mouthRatio > 2.5) {
      score += 0.3;
    }
    
    // If mouth is turned down, reduce happiness score
    if (isMouthTurnedDown) {
      score *= 0.3; // Significant reduction
    }
    
    return Math.min(0.95, score); // Cap at 0.95 - happiness can be very clear
  };
  // Completely new emotion detection algorithm focusing on better detection of all emotions
  const calculateEmotionsFromLandmarks = (landmarks) => {
    if (!landmarks || landmarks.length === 0) {
      return {
        neutral: 0.7,
        happy: 0.1,
        sad: 0.1,
        angry: 0.05,
        surprised: 0.05
      };
    }
    
    // Define key facial landmark indices
    // These are for MediaPipe's facemesh model
    const leftEyeTop = landmarks[159] || landmarks[27];
    const leftEyeBottom = landmarks[145] || landmarks[23];
    const rightEyeTop = landmarks[386] || landmarks[257];
    const rightEyeBottom = landmarks[374] || landmarks[253];
    
    const leftEyebrowOuter = landmarks[46] || landmarks[70];
    const leftEyebrowInner = landmarks[105] || landmarks[63];
    const rightEyebrowOuter = landmarks[276] || landmarks[291];
    const rightEyebrowInner = landmarks[334] || landmarks[293];
    
    const upperLip = landmarks[13] || landmarks[0];
    const lowerLip = landmarks[14] || landmarks[17];
    const leftMouthCorner = landmarks[61] || landmarks[308];
    const rightMouthCorner = landmarks[291] || landmarks[291];
    
    // Calculate distances for emotion assessment
    // Eye openness
    const leftEyeOpenness = distance(leftEyeTop, leftEyeBottom);
    const rightEyeOpenness = distance(rightEyeTop, rightEyeBottom);
    const avgEyeOpenness = (leftEyeOpenness + rightEyeOpenness) / 2;
    
    // Eyebrow position relative to eyes
    const leftEyebrowHeight = leftEyebrowOuter[1] - leftEyeTop[1];
    const rightEyebrowHeight = rightEyebrowOuter[1] - rightEyeTop[1];
    
    // Mouth measurements
    const mouthWidth = distance(leftMouthCorner, rightMouthCorner);
    const mouthHeight = distance(upperLip, lowerLip);
    const mouthRatio = mouthWidth / (mouthHeight || 0.1); // Avoid division by zero
    
    // Mouth corner position (for detecting smiles vs frowns)
    const mouthCornerAvgY = (leftMouthCorner[1] + rightMouthCorner[1]) / 2;
    const mouthMiddleY = (upperLip[1] + lowerLip[1]) / 2;
    const isMouthTurnedDown = mouthCornerAvgY > mouthMiddleY;
    
    // Mouth corner difference (asymmetry can indicate some emotions)
    const mouthCornerDiff = Math.abs(leftMouthCorner[1] - rightMouthCorner[1]);
    
    // Eyebrow angles for detecting anger
    const leftEyebrowAngle = getAngle(leftEyebrowOuter, leftEyebrowInner);
    const rightEyebrowAngle = getAngle(rightEyebrowInner, rightEyebrowOuter);
    const eyebrowsAngledInward = leftEyebrowAngle < -15 && rightEyebrowAngle > 15;
    
    // Surprise indicators
    const eyebrowRaised = leftEyebrowHeight > 10 && rightEyebrowHeight > 10;
    const eyesWideOpen = avgEyeOpenness > 15;
    const isMouthOpen = mouthHeight > 10;
    
    // Anger indicators
    const eyebrowLowered = leftEyebrowHeight < -5 && rightEyebrowHeight < -5;
    
    // Calculate individual emotion scores
    const sadness = calculateSadnessScore(isMouthTurnedDown, mouthCornerDiff, mouthRatio);
    const anger = calculateAngerScore(eyebrowLowered, eyebrowsAngledInward, leftEyebrowAngle, rightEyebrowAngle);
    const surprise = calculateSurpriseScore(eyebrowRaised, eyesWideOpen, isMouthOpen);
    const happiness = calculateHappinessScore(mouthRatio, isMouthTurnedDown);
    
    // Calculate neutral as inverse of other emotions
    let emotionSum = sadness + anger + surprise + happiness;
    const neutral = Math.max(0.1, 1 - emotionSum); // Ensure at least 10% neutral
    
    // Create emotions object
    const emotions = {
      neutral,
      happy: happiness,
      sad: sadness,
      angry: anger,
      surprised: surprise
    };
    
    // Normalize emotion values to ensure they sum to 1
    const total = Object.values(emotions).reduce((sum, val) => sum + val, 0);
    Object.keys(emotions).forEach(key => {
      emotions[key] = emotions[key] / total;
    });
    
    return emotions;
  };
  
  // Helper function to calculate distance between two points
  const distance = (point1, point2) => {
    if (!point1 || !point2) return 0;
    return Math.sqrt(
      Math.pow(point2[0] - point1[0], 2) + 
      Math.pow(point2[1] - point1[1], 2)
    );
  };
  
  // Helper function to calculate angle between two points
  const getAngle = (point1, point2) => {
    if (!point1 || !point2) return 0;
    return Math.atan2(point2[1] - point1[1], point2[0] - point1[0]) * 180 / Math.PI;
  };
  // Initialize everything
  useEffect(() => {
    loadLibraries();
    
    // Setup detection interval
    const detectionInterval = setInterval(() => {
      if (!isDetecting && modelsLoaded) {
        runDetection();
      }
    }, 100); // Run detection every 100ms if not already detecting
    
    // Clean up on component unmount
    return () => {
      clearInterval(detectionInterval);
      // Stop video and release camera
      if (videoRef.current && videoRef.current.srcObject) {
        const tracks = videoRef.current.srcObject.getTracks();
        tracks.forEach(track => track.stop());
      }
    };
  }, [runDetection, isDetecting, modelsLoaded, loadLibraries]);

  // Define a function to fetch mood history
  const fetchMoodHistory = async () => {
    try {
      const response = await fetch('/api/mood-logs');
      if (response.ok) {
        const data = await response.json();
        setHistory(data);
      } else {
        console.error('Failed to fetch mood history');
      }
    } catch (err) {
      console.error('Error fetching mood history:', err);
    }
  };

  // Handle saving mood to backend
  const handleSaveMood = async () => {
    try {
      const moodData = {
        mood,
        emotionScores,
        notes,
        timestamp: new Date().toISOString()
      };
      
      // Simple client-side validation
      if (!mood) {
        alert('Please wait for emotion detection before saving');
        return;
      }
      
      const response = await fetch('/api/mood-logs', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(moodData)
      });
      
      if (response.ok) {
        // Clear notes field after successful save
        setNotes('');
        // Refresh history
        fetchMoodHistory();
        alert('Mood saved successfully!');
      } else {
        alert('Failed to save mood. Please try again.');
      }
    } catch (err) {
      console.error('Error saving mood:', err);
      alert('Error saving mood. Please try again.');
    }
  };
  
  // Helper functions for UI
  const getEmotionColor = (emotion) => {
    const colors = {
      happy: '#FFD700',  // Gold
      sad: '#1E90FF',    // Dodger Blue
      angry: '#FF4500',  // Orange Red
      surprised: '#9932CC', // Dark Orchid
      neutral: '#7CFC00'  // Lawn Green
    };
    return colors[emotion] || '#FFFFFF';
  };
  
  const getEmotionIcon = (emotion) => {
    const icons = {
      happy: '😊',
      sad: '😢',
      angry: '😠',
      surprised: '😮',
      neutral: '😐'
    };
    return icons[emotion] || '❓';
  };
  
  // Helper function for drawing hand landmarks
  const drawHandLandmarks = (ctx, keypoints, color) => {
    if (!keypoints || !ctx) return;
    
    // Set drawing styles
    ctx.fillStyle = color;
    ctx.strokeStyle = color;
    ctx.lineWidth = 2;
    
    // Draw each keypoint
    keypoints.forEach(point => {
      ctx.beginPath();
      ctx.arc(point.x, point.y, 3, 0, 2 * Math.PI);
      ctx.fill();
                  });
                  
                  // Draw connections between landmarks
                  if (keypoints && keypoints.length >= 21) {
                    // Draw hand skeleton
                    const connections = [
                      [0, 1], [1, 2], [2, 3], [3, 4], // thumb
                      [0, 5], [5, 6], [6, 7], [7, 8], // index finger
                      [0, 9], [9, 10], [10, 11], [11, 12], // middle finger
                      [0, 13], [13, 14], [14, 15], [15, 16], // ring finger
                      [0, 17], [17, 18], [18, 19], [19, 20] // pinky
                    ];
                    
                    connections.forEach(([i, j]) => {
                      if (keypoints[i] && keypoints[j]) {
                        ctx.beginPath();
                        ctx.moveTo(keypoints[i].x, keypoints[i].y);
                        ctx.lineTo(keypoints[j].x, keypoints[j].y);
                        ctx.stroke();
                      }
                    });
                  }
                }
              });
            }
          } else {
            // No hands detected
            setHandDetails({
              handsDetected: 0,
              handedness: [],
              keypoints: [],
              confidence: [],
              fallbackMode: false
            });
          }
          
          // Detection cycle complete
          setIsDetecting(false);
        })
        .catch(error => {
          console.error('AI detection error:', error);
          handleFallbackDetection();
        });
      } else {
        // Use fallback detection if models aren't available
        handleFallbackDetection();
      }
    } catch (err) {
      console.error('Detection error:', err);
      handleFallbackDetection();
    }
  }, [faceModel, handModel, isVideoReady, showLandmarks, handleFallbackDetection, canvasRef, setEmotionScores, setMood, setFaceDetails, setHandDetails, setIsDetecting, calculateEmotionsFromLandmarks]);
  
  // Helper functions for emotion detection
  const calculateSadnessScore = (isMouthTurnedDown, mouthCornerDiff, mouthRatio) => {
    let score = 0;
    
    // Strongly weight mouth turned down
    if (isMouthTurnedDown) {
      score += 0.6;
    }
    
    // Add score for uneven mouth corners (sadness often causes asymmetry)
    if (mouthCornerDiff > 3) {
      score += 0.3;
    } else if (mouthCornerDiff > 1.5) {
      score += 0.15;
    }
    
    // Low mouth ratio can indicate sadness (not smiling)
    if (mouthRatio < 2.0) {
      score += 0.2;
    }
    
    return Math.min(0.9, score); // Cap at 0.9
  };
  
  const calculateAngerScore = (eyebrowLowered, eyebrowsAngledInward, leftAngle, rightAngle) => {
    let score = 0;
    
    // Strongly weight lowered eyebrows
    if (eyebrowLowered) {
      score += 0.5;
    }
    
    // Strongly weight angled eyebrows (classic anger expression)
    if (eyebrowsAngledInward) {
      score += 0.4;
    } else if (leftAngle < -10 || rightAngle > 10) {
      // Partial eyebrow angling
      score += 0.25;
    }
    
    return Math.min(0.9, score); // Cap at 0.9
  };
  
  const calculateSurpriseScore = (eyebrowRaised, eyesWideOpen, isMouthOpen) => {
    let score = 0;
    
    // Strongly weight raised eyebrows
    if (eyebrowRaised) {
      score += 0.4;
    }
    
    // Wide eyes are a strong indicator
    if (eyesWideOpen) {
      score += 0.3;
    }
    
    // Open mouth completes the surprised expression
    if (isMouthOpen) {
      score += 0.3;
    }
    
    return Math.min(0.9, score); // Cap at 0.9
  };
  
  const calculateHappinessScore = (mouthRatio, isMouthTurnedDown) => {
    let score = 0;
    
    // Only count as smiling if mouth ratio is high enough
    if (mouthRatio > 3.5) {
      score += 0.6;
    } else if (mouthRatio > 2.8) {
      score += 0.4;
    } else if (mouthRatio > 2.2) {
      score += 0.2;
    }
    
    // Penalty if mouth is turned down (can't be happy with a frown)
    if (isMouthTurnedDown) {
      score = Math.max(0, score - 0.4);
    }
    
    return Math.min(0.8, score); // Cap at 0.8 to prevent bias toward happiness
  };
  
  // Completely new emotion detection algorithm focusing on better detection of all emotions
  const calculateEmotionsFromLandmarks = (landmarks) => {
    // Debug variables to help visualize what's being detected
    window.debugFaceMetrics = {};
    
    // Default emotion distribution - now with MUCH LOWER initial happy value
    let emotions = {
      neutral: 0.25,
      happy: 0.05, // Dramatically reduced default for happy
      sad: 0.25,   // Increased default for sad
      angry: 0.25, // Increased default for angry
      surprised: 0.20 // Increased default for surprised
    };
    
    if (!landmarks || landmarks.length < 20) {
      return emotions;
    }
    
    try {
      // Key facial points (using multiple points for more reliable detection)
      // Mouth points
      const mouthLeft = landmarks[61] || landmarks[0];
      const mouthRight = landmarks[291] || landmarks[1];
      const upperLip = landmarks[13] || landmarks[2];
      const lowerLip = landmarks[14] || landmarks[3];
      const mouthCenter = landmarks[0] || landmarks[4];
      
      // Eye points
      const leftEye = landmarks[159] || landmarks[5];
      const rightEye = landmarks[386] || landmarks[6];
      const leftEyeTop = landmarks[159] || landmarks[7];
      const leftEyeBottom = landmarks[145] || landmarks[8];
      const rightEyeTop = landmarks[386] || landmarks[9];
      const rightEyeBottom = landmarks[374] || landmarks[10];
      
      // Eyebrow points
      const leftEyebrowOuter = landmarks[65] || landmarks[11];
      const leftEyebrowInner = landmarks[105] || landmarks[12];
      const rightEyebrowOuter = landmarks[295] || landmarks[13];
      const rightEyebrowInner = landmarks[334] || landmarks[14];
      
      // More points for better detection
      const noseTip = landmarks[1] || landmarks[15];
      const leftCheek = landmarks[187] || landmarks[16];
      const rightCheek = landmarks[411] || landmarks[17];
      const forehead = landmarks[151] || landmarks[18];
      
      // ---------- CALCULATE CORE METRICS ----------
      
      // 1. MOUTH MEASUREMENTS
      // Mouth width (horizontal distance)
      const mouthWidth = Math.sqrt(
        Math.pow(mouthRight[0] - mouthLeft[0], 2) + 
        Math.pow(mouthRight[1] - mouthLeft[1], 2)
      );
      
      // Mouth height (vertical opening)
      const mouthHeight = Math.sqrt(
        Math.pow(upperLip[0] - lowerLip[0], 2) + 
        Math.pow(upperLip[1] - lowerLip[1], 2)
      );
      
      // Mouth corner height difference (key for sadness)
      const mouthCornerDiff = Math.abs(mouthLeft[1] - mouthRight[1]);
      
      // Is the mouth turned down? (critical for sadness detection)
      const isMouthTurnedDown = (mouthLeft[1] > mouthCenter[1]) && (mouthRight[1] > mouthCenter[1]);
      
      // Is the mouth open? (important for surprise)
      const isMouthOpen = mouthHeight > (mouthWidth * 0.3);
      
      // Smile ratio - width to height (smaller = less smiling)
      const mouthRatio = mouthWidth / (mouthHeight || 1);
      
      // 2. EYEBROW MEASUREMENTS
      // Distance between eyebrow and eye (key for surprise and anger)
      const leftEyebrowHeight = Math.abs(leftEyebrowOuter[1] - leftEye[1]);
      const rightEyebrowHeight = Math.abs(rightEyebrowOuter[1] - rightEye[1]);
      
      // Are eyebrows raised? (surprise)
      const eyebrowRaised = (leftEyebrowHeight > 25) || (rightEyebrowHeight > 25);
      
      // Are eyebrows lowered/furrowed? (anger)
      const eyebrowLowered = (leftEyebrowHeight < 15) || (rightEyebrowHeight < 15);
      
      // Eyebrow angle (furrowed brows for anger)
      const leftEyebrowAngle = Math.atan2(
        leftEyebrowOuter[1] - leftEyebrowInner[1],
        leftEyebrowOuter[0] - leftEyebrowInner[0]
      ) * (180 / Math.PI);
      
      const rightEyebrowAngle = Math.atan2(
        rightEyebrowInner[1] - rightEyebrowOuter[1],
        rightEyebrowInner[0] - rightEyebrowOuter[0]
      ) * (180 / Math.PI);
      
      // Are eyebrows angled inward? (anger)
      const eyebrowsAngledInward = (leftEyebrowAngle < -15) && (rightEyebrowAngle > 15);
      
      // 3. EYE MEASUREMENTS
      // Eye openness (surprise)
      const leftEyeOpenness = Math.abs(leftEyeTop[1] - leftEyeBottom[1]);
      const rightEyeOpenness = Math.abs(rightEyeTop[1] - rightEyeBottom[1]);
      
      // Are eyes wide open? (surprise)
      const eyesWideOpen = (leftEyeOpenness > 15) || (rightEyeOpenness > 15);
      
      // ---------- EMOTION DETECTION LOGIC ----------
      
      // DEBUGGING - store metrics for visualization
      window.debugFaceMetrics = {
        mouthRatio,
        mouthCornerDiff,
        isMouthTurnedDown,
        isMouthOpen,
        eyebrowRaised,
        eyebrowLowered,
        eyebrowsAngledInward,
        eyesWideOpen,
        leftEyebrowAngle,
        rightEyebrowAngle
      };
      
      // CRITICAL FACIAL EXPRESSION DETECTION
      // Using stronger indicators and clear thresholds for each emotion
      
      // 1. SADNESS - Detect with high priority
      // Key indicators: Mouth corners turned down, slight frown
      const sadnessScore = calculateSadnessScore(isMouthTurnedDown, mouthCornerDiff, mouthRatio);
      
      // 2. ANGER - Also detect with high priority
      // Key indicators: Lowered/furrowed eyebrows, eyebrows angled inward
      const angerScore = calculateAngerScore(eyebrowLowered, eyebrowsAngledInward, leftEyebrowAngle, rightEyebrowAngle);
      
      // 3. SURPRISE - Detect with high priority
      // Key indicators: Raised eyebrows, wide eyes, open mouth
      const surpriseScore = calculateSurpriseScore(eyebrowRaised, eyesWideOpen, isMouthOpen);
      
      // 4. HAPPINESS - Only detect when clearly smiling
      // Key indicator: Wide mouth with corners up (high mouth ratio)
      const happinessScore = calculateHappinessScore(mouthRatio, isMouthTurnedDown);
      
      // 5. NEUTRALITY - Default state when other emotions aren't strong
      const neutralScore = 0.4 - (sadnessScore + angerScore + surpriseScore + happinessScore) / 2;
      
      // Assign scores to emotions object
      emotions.sad = Math.max(0.1, sadnessScore);
      emotions.angry = Math.max(0.1, angerScore);
      emotions.surprised = Math.max(0.1, surpriseScore);
      emotions.happy = Math.min(0.7, happinessScore); // Cap happiness to prevent bias
      emotions.neutral = Math.max(0.05, neutralScore);
      
      // Add some noise to make emotions more dynamic even with subtle changes
      Object.keys(emotions).forEach(key => {
        // Add small random variation to create more responsive changes
        emotions[key] = emotions[key] + (Math.random() * 0.05 - 0.025);
        // Ensure values stay positive
        emotions[key] = Math.max(0, emotions[key]);
      });
      
      // Normalize the values to sum to 1
      const total = Object.values(emotions).reduce((sum, val) => sum + val, 0);
      Object.keys(emotions).forEach(key => {
        emotions[key] = emotions[key] / total;
      });
      
      return emotions;
    } catch (err) {
      console.error('Error calculating emotions:', err);
      return emotions;
    }
  };
  
  // AI detection logic is contained within the runDetection function
  
  // Helper function for drawing hand landmark connections
  const drawPath = (ctx, points) => {
    if (!points || points.length < 2) return;
    
    ctx.beginPath();
    ctx.moveTo(points[0].x, points[0].y);
    
    for (let i = 1; i < points.length; i++) {
      if (points[i] && typeof points[i].x === 'number') {
        ctx.lineTo(points[i].x, points[i].y);
      }
    }
    
    ctx.stroke();
  };

  // Initialize everything
  useEffect(() => {
    loadLibraries();
    
    // Store video reference at effect start to use in cleanup
    const videoRefCurrent = videoRef.current;
    
    return () => {
      if (detectionIntervalRef.current) {
        clearInterval(detectionIntervalRef.current);
      }
      if (videoRefCurrent && videoRefCurrent.srcObject) {
        // Using stored reference to avoid cleanup issues with changing refs
        const stream = videoRefCurrent.srcObject;
        if (stream) {
          const tracks = stream.getTracks();
          tracks.forEach(track => track.stop());
        }
      }
    };
  }, [loadLibraries]);

  // Set up detection interval with much faster refresh rate for ultra-responsive emotion detection
  useEffect(() => {
    if (isVideoReady) {
      // Ultra-fast interval (50ms) for extremely responsive emotion detection
      detectionIntervalRef.current = setInterval(runDetection, 50);
      return () => {
        if (detectionIntervalRef.current) {
          clearInterval(detectionIntervalRef.current);
        }
      };
    }
  }, [isVideoReady, runDetection]);

  // Load history from API on component mount
  useEffect(() => {
    // Define a function to fetch mood history
    function fetchMoodHistory() {
      fetch('http://localhost:5000/api/moods/history')
        .then(response => {
          if (response.ok) {
            return response.json();
          }
          console.error('Failed to load mood history');
          return [];
        })
        .then(data => {
          setHistory(data);
        })
        .catch(err => {
          console.error('Error loading mood history:', err);
        });
    }

    fetchMoodHistory();
  }, []);

  const handleSaveMood = () => {
    if (!mood) return;
    
    const moodData = { 
      mood, 
      notes, 
      emotionScores,
      faceDetails,
      handDetails,
      timestamp: new Date().toISOString() 
    };
    
    fetch('http://localhost:5000/api/moods/log', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(moodData),
    })
    .then(response => {
      if (response.ok) {
        return response.json().then(savedData => {
          setHistory(prev => [savedData, ...prev.slice(0, 19)]); // Keep last 20 entries
          setNotes('');
          
          // Show success message
          alert('Mood saved successfully! 🎉');
        });
      } else {
        return response.json().then(errorData => {
          alert(`Failed to save: ${errorData.message || 'Unknown error'}`);
        });
      }
    })
    .catch(err => {
      console.error('Error saving mood:', err);
      alert('Failed to save mood. Please try again.');
    });
  };

  const getEmotionColor = (emotion) => {
    const colors = {
      happy: 'success',
      sad: 'info',
      angry: 'danger',
      surprised: 'warning',
      neutral: 'secondary'
    };
    return colors[emotion] || 'primary';
  };

  const getEmotionIcon = (emotion) => {
    const icons = {
      happy: '😊',
      sad: '😢',
      angry: '😠',
      surprised: '😮',
      neutral: '😐'
    };
    return icons[emotion] || '🤔';
  };

  if (error) {
    return (
      <div className="container mt-5" style={{ background: '#121212', minHeight: '100vh' }}>
        <div className="alert" role="alert" style={{ background: '#1E1E1E', color: '#E1C4FF', border: '1px solid #6A1B9A' }}>
          <h4 className="alert-heading" style={{ color: '#9C27B0' }}>Notice</h4>
          <p>{error}</p>
          <button className="btn" onClick={() => window.location.reload()} 
            style={{ background: '#4A148C', color: '#FFFFFF', borderColor: '#6A1B9A' }}>
            Refresh Page
          </button>
        </div>
      </div>
    );
  }

  return (
    <div className="container-fluid py-4" style={{ 
      background: 'linear-gradient(135deg, #0D0221 0%, #190B33 50%, #240D57 100%)', 
      minHeight: '100vh', 
      color: '#E1C4FF'
    }}>
      <div className="container">
        <div className="row mb-4">
          <div className="col-12 text-center">
            <h1 className="display-4 mb-3 fw-bold" style={{ color: '#FFFFFF' }}>
              🧠 Advanced Mood & Gesture Tracker
            </h1>
            <p className="lead" style={{ color: '#FFFFFF' }}>
              Real-time emotion detection using AI-powered facial and hand analysis
            </p>
          </div>
        </div>
        
        <div className="row g-4">
          {/* Video Section */}
          <div className="col-lg-8">
            <div className="card shadow-lg border-0 h-100" style={{ background: '#1E1E1E', borderColor: '#4A148C' }}>
              <div className="card-header text-white" style={{ background: '#4A148C' }}>
                <h5 className="card-title mb-0">
                  📹 Live Detection
                  {isDetecting && <span className="spinner-border spinner-border-sm ms-2" role="status"></span>}
                </h5>
              </div>
              <div className="card-body">
                <div className="position-relative mb-3">
                  <video 
                    ref={videoRef} 
                    autoPlay 
                    playsInline
                    muted
                    className="w-100 rounded"
                    style={{ 
                      maxHeight: '480px', 
                      objectFit: 'cover',
                      border: '2px solid #6A1B9A',
                      boxShadow: '0 0 20px rgba(106, 27, 154, 0.5)'
                    }}
                  />
                  <canvas 
                    ref={canvasRef}
                    className="position-absolute top-0 start-0 w-100 h-100"
                    style={{ pointerEvents: 'none', maxHeight: '480px' }}
                  />
                </div>
                
                <div className="d-flex flex-wrap align-items-center gap-3">
                  <div className="form-check form-switch">
                    <input 
                      className="form-check-input" 
                      type="checkbox" 
                      id="landmarksToggle"
                      checked={showLandmarks}
                      onChange={(e) => setShowLandmarks(e.target.checked)}
                      style={{ backgroundColor: showLandmarks ? '#9C27B0' : '#333' }}
                    />
                    <label className="form-check-label" htmlFor="landmarksToggle" style={{ color: '#E1C4FF' }}>
                      Show Landmarks
                    </label>
                  </div>
                  
                  <span className="badge" style={{
                    backgroundColor: isVideoReady ? '#7B1FA2' : '#D32F2F',
                    color: 'white',
                    border: '1px solid ' + (isVideoReady ? '#9C27B0' : '#F44336'),
                    padding: '8px 12px'
                  }}>
                    📷 Video: {isVideoReady ? 'Ready' : 'Loading'}
                  </span>
                  
                  <span className="badge" style={{
                    backgroundColor: modelsLoaded ? '#7B1FA2' : '#EF6C00',
                    color: 'white',
                    border: '1px solid ' + (modelsLoaded ? '#9C27B0' : '#FF9800'),
                    padding: '8px 12px'
                  }}>
                    🤖 AI Models: {modelsLoaded ? 'Loaded' : 'Loading'}
                  </span>
                </div>
              </div>
            </div>
          </div>

          {/* Analysis Results */}
          <div className="col-lg-4">
            <div className="row g-3">
              {/* Current Emotion */}
              <div className="col-12">
                <div className="card shadow border-0 h-100" style={{ background: '#1E1E1E', borderColor: '#4A148C' }}>
                  <div className="card-header text-white" style={{ background: '#6A1B9A' }}>
                    <h5 className="card-title mb-0">🎭 Current Emotion</h5>
                  </div>
                  <div className="card-body text-center">
                    <div className="display-6 mb-3">
                      {getEmotionIcon(mood)}
                    </div>
                    <h3 className="text-capitalize mb-3" style={{ color: '#FFFFFF', fontSize: '28px', textShadow: '0 0 5px rgba(0,0,0,0.7)' }}>
                      {mood || 'Analyzing...'}
                    </h3>
                    
                    {Object.keys(emotionScores).length > 0 && (
                      <div className="mt-3">
                        {Object.entries(emotionScores)
                          .sort(([,a], [,b]) => b - a)
                          .slice(0, 3)
                          .map(([emotion, score]) => (
                          <div key={emotion} className="mb-2">
                            <div className="d-flex justify-content-between align-items-center mb-1">
                              <small className="text-capitalize fw-semibold" style={{ color: '#FFFFFF', fontSize: '14px' }}>
                                {getEmotionIcon(emotion)} {emotion}
                              </small>
                              <small style={{ color: '#FFFFFF', fontSize: '14px' }}>{(score * 100).toFixed(0)}%</small>
                            </div>
                            <div className="progress" style={{ height: '6px', backgroundColor: '#333' }}>
                              <div 
                                className={`progress-bar bg-${getEmotionColor(emotion)}`}
                                style={{ width: `${score * 100}%` }}
                              ></div>
                            </div>
                          </div>
                        ))}
                      </div>
                    )}
                  </div>
                </div>
              </div>

              {/* Detection Details */}
              {(faceDetails || handDetails) && (
                <div className="col-12">
                  <div className="card shadow border-0" style={{ background: '#1E1E1E', borderColor: '#4A148C' }}>
                    <div className="card-header text-white" style={{ background: '#512DA8' }}>
                      <h6 className="card-title mb-0">🔍 Detection Details</h6>
                    </div>
                    <div className="card-body">
                      {faceDetails && (
                        <div className="mb-3">
                          <h6 style={{ color: '#CE93D8' }}>👤 Face Analysis</h6>
                          <ul className="list-unstyled mb-0" style={{ fontSize: '15px', color: '#FFFFFF' }}>
                            <li>✅ Faces: {faceDetails.facesDetected}</li>
                            <li>📍 Keypoints: {faceDetails.keypoints}</li>
                            <li>🎯 Accuracy: {(faceDetails.confidence * 100).toFixed(1)}%</li>
                          </ul>
                        </div>
                      )}
                      
                      {handDetails && (
                        <div>
                          <h6 style={{ color: '#BA68C8' }}>✋ Hand Analysis</h6>
                          <ul className="list-unstyled mb-0" style={{ fontSize: '15px', color: '#FFFFFF' }}>
                            <li>🤚 Hands: {handDetails.handsDetected}</li>
                            {handDetails.fallbackMode ? (
                              <li>No hands detected in current frame</li>
                            ) : (
                              handDetails.handedness && handDetails.handedness.length > 0 && 
                              handDetails.handedness.map((hand, index) => (
                                <li key={index}>
                                  {index === 0 ? '👈' : '👉'} {hand} 
                                  {handDetails.keypoints && handDetails.keypoints[index] !== undefined ? 
                                    `(${handDetails.keypoints[index]} points)` : ''}
                                </li>
                              ))
                            )}
                          </ul>
                        </div>
                      )}
                    </div>
                  </div>
                </div>
              )}
            </div>
          </div>
        </div>

        {/* Notes Section */}
        <div className="row mt-4">
          <div className="col-12">
            <div className="card shadow border-0" style={{ background: '#1E1E1E', borderColor: '#4A148C' }}>
              <div className="card-header text-white" style={{ background: '#7B1FA2' }}>
                <h5 className="card-title mb-0">📝 Add Notes & Save Entry</h5>
              </div>
              <div className="card-body" style={{ background: '#1E1E1E', color: '#E1C4FF' }}>
                <div className="mb-3">
                  <textarea
                    className="form-control"
                    rows="3"
                    placeholder="How are you feeling? Add your thoughts about your current mood..."
                    value={notes}
                    onChange={(e) => setNotes(e.target.value)}
                    style={{ 
                      backgroundColor: '#2C2C2C', 
                      color: '#FFFFFF', 
                      border: '1px solid #6A1B9A',
                      fontSize: '16px'
                    }}
                  />
                </div>
                <button 
                  className="btn btn-lg"
                  onClick={handleSaveMood} 
                  disabled={!mood}
                  style={{
                    backgroundColor: '#6A1B9A',
                    color: '#FFFFFF',
                    border: '1px solid #9C27B0',
                    boxShadow: '0 4px 20px rgba(156, 39, 176, 0.3)'
                  }}
                >
                  💾 Save Mood Entry
                </button>
              </div>
            </div>
          </div>
        </div>

        {/* History Section */}
        {history.length > 0 && (
          <div className="row mt-4">
            <div className="col-12">
              <div className="card shadow border-0" style={{ background: '#1E1E1E', borderColor: '#4A148C' }}>
                <div className="card-header text-white" style={{ background: '#311B92' }}>
                  <h5 className="card-title mb-0">📊 Mood History</h5>
                </div>
                <div className="card-body" style={{ maxHeight: '400px', overflowY: 'auto', backgroundColor: '#1E1E1E' }}>
                  <div className="row g-3">
                    {history.map((log, index) => (
                      <div key={index} className="col-md-6 col-lg-4">
                        <div className="card h-100" style={{ 
                          backgroundColor: '#2C2C2C', 
                          borderWidth: '2px',
                          borderStyle: 'solid',
                          borderColor: log.mood === 'happy' ? '#8E24AA' : 
                                       log.mood === 'sad' ? '#5E35B1' : 
                                       log.mood === 'angry' ? '#D81B60' : 
                                       log.mood === 'surprised' ? '#7B1FA2' : '#4527A0'
                        }}>
                          <div className="card-body">
                            <div className="d-flex align-items-center mb-2">
                              <span className="fs-4 me-2">{getEmotionIcon(log.mood)}</span>
                              <h6 className="text-capitalize mb-0" style={{ 
                                color: log.mood === 'happy' ? '#CE93D8' : 
                                       log.mood === 'sad' ? '#B39DDB' : 
                                       log.mood === 'angry' ? '#F48FB1' : 
                                       log.mood === 'surprised' ? '#E1BEE7' : '#B388FF'
                              }}>
                                {log.mood}
                              </h6>
                            </div>
                            
                            {log.notes && (
                              <p className="card-text mb-2" style={{ color: '#FFFFFF', fontSize: '14px' }}>
                                "{log.notes}"
                              </p>
                            )}
                            
                            <div style={{ color: '#FFFFFF', fontSize: '14px' }}>
                              <div>📅 {new Date(log.timestamp).toLocaleDateString()}</div>
                              <div>🕐 {new Date(log.timestamp).toLocaleTimeString()}</div>
                              <div className="mt-1">
                                👤 {log.faceDetails ? '✅' : '❌'} | 
                                ✋ {log.handDetails ? log.handDetails.handsDetected : 0}
                              </div>
                            </div>
                          </div>
                        </div>
                      </div>
                    ))}
                  </div>
                </div>
              </div>
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

export default MoodTracker;
